# 无人机艇协同搜索强化学习高级框架 (Optimized)

## 1. 框架定位

这是一个为“无人机艇协同搜索”竞赛场景设计的、稳定且经过优化的**高级强化学习框架 (Advanced, Optimized Framework)**。

本框架的核心目标是为算法研究者和开发者提供一个功能强大的起点。它不仅包含一个功能完备的仿真环境和一个经过验证的IPPO算法实现，更在此基础上集成了**智能体协同机制**、**动态课程学习**和**复杂的奖励策略**。开发者可以轻松地在此框架上：

-   **测试新算法**: 替换`ippo.py`为自己的算法实现。
-   **对比性能**: 将新算法的表现与本框架提供的IPPO基线和高级功能进行量化比较。
-   **实验新策略**: 调整奖励函数、观测空间或环境配置，研究不同策略对协同行为的影响。
-   **验证优化效果**: 使用 `test_optimizations.py` 独立验证各项功能的性能表现。

## 2. 快速入门

### 2.1 安装依赖

```bash
pip install -r requirements.txt
```

### 2.2 运行训练

```bash
python main.py
```

默认配置已设为无渲染的高速训练模式。若需可视化，请修改`config.py`。

### 2.3 (可选) 运行优化项验证测试

```bash
python test_optimizations.py
```

该脚本可以独立测试和验证边界检测、协同机制、奖励系统等各项优化功能。

## 3. 项目文件结构解析

| 文件名                  | 主要职责                                                     |
| :---------------------- | :----------------------------------------------------------- |
| `main.py`               | **主程序入口**。负责组织训练循环，调用环境和算法，并实现**课程学习**策略。 |
| `environment.py`        | **核心仿真环境**。基于Gymnasium构建，定义状态、动作、奖励，并实现了**高级检测、协同与奖励逻辑**。 |
| `agents.py`             | **智能体物理与协同模型**。定义了UAV、USV和Target的属性、基础移动以及**智能体间的协同交互机制**。 |
| `ippo.py`               | **基线算法实现**。提供了一个可运行的IPPO算法作为性能参考。   |
| `config.py`             | **中央配置文件**。所有超参数、**高级奖励权重**和物理参数都在此定义。 |
| `evaluator.py`          | **性能评估器**。用于计算和输出标准化的性能指标（如成功率、覆盖率）。 |
| `test_optimizations.py` | **新增的验证脚本**。用于独立测试和验证边界检测、协同、奖励系统等各项优化功能。 |
| `README.md`             | **开发文档** (本文)。                                        |
| `requirements.txt`      | **项目依赖库**。                                             |


## 4. 核心特性与优化

此框架在基线版的基础上，集成了一系列先进的算法和策略优化，旨在解决复杂场景下的协同搜索问题。

### 4.1 智能体协同机制

为了实现智能体之间的高效协作，`agents.py` 和 `environment.py` 中实现了专门的协同机制：

-   **协同角色分配**: 智能体可以扮演不同角色（例如，探索者、追踪者），动态调整其行为策略。
-   **信息共享**: 智能体在通信范围内可以共享关键目标信息，避免重复劳动，形成合力。
-   **协同奖励**: `config.py` 中新增 `REWARD_COORDINATION` 参数，鼓励智能体共同确认和包围目标。

### 4.2 动态课程学习

为了提高训练效率和最终性能，`main.py` 和 `environment.py` 引入了课程学习策略：

-   **难度动态调整**: 训练从简单任务开始（例如，目标较少、速度较慢），随着智能体能力的提升，通过 `adjust_difficulty()` 函数逐步增加任务难度（如生成高速目标、边界目标）。
-   **针对性训练**: 支持通过配置生成特定类型的目标（例如，`boundary_prob`），对智能体的特定能力进行强化训练。

### 4.3 扩展的观测空间与奖励函数

为了让智能体能够理解更复杂的环境并做出更优的决策，我们对观测和奖励进行了大幅扩展：

-   **57维观测空间**: 观测空间从44维扩展至57维，新增了 `目标优先级` 和 `协同状态` 等关键信息，详见 **环境详解** 部分。
-   **专项奖励设计**: 在 `config.py` 中增加了多个新的奖励项，以引导智能体学习特定高级技能，例如：
    -   `REWARD_BOUNDARY_TARGET`: 鼓励探测地图边界的“边缘”目标。
    -   `REWARD_HIGH_SPEED_TARGET`: 激励追捕和拦截高速移动的目标。
    -   `REWARD_EARLY_DETECTION`: 对在任务早期就发现目标的行为给予额外奖励。

## 5. 如何扩展：实现并测试你的新算法

本框架的核心设计思想是易于扩展。只需遵循以下两步，即可将你的算法集成到框架中。

### 步骤1: 实现算法接口

你需要创建一个新的Python文件（例如 `my_maddpg.py`），并在其中实现一个与`PPOAgent`有相同接口的类。

**算法接口规范**:

```python
class YourAlgorithm:
    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_type):
        """
        构造函数。即使你的算法不需要所有参数，也请保持签名一致以便于替换。
        - state_dim: 状态空间的维度 (当前为57)
        - action_dim: 动作空间的维度 (离散:11, 连续:2)
        - action_type: 'discrete' 或 'continuous'
        """
        # ... 你的初始化代码 ...
        pass

    def select_action(self, state):
        """
        根据当前状态选择一个动作。
        - state: 一个np.array，代表单个智能体的观测值。
        - return: 返回一个动作。对于离散空间，返回一个int；对于连续空间，返回一个np.array。
        """
        # ... 你的动作选择逻辑 ...
        pass

    def update(self):
        """
        执行一次策略更新（训练）。
        框架会在每个 UPDATE_TIMESTEPS 调用此方法。
        你需要在此方法内管理你的经验回放缓冲区。
        """
        # ... 你的训练逻辑 ...
        pass
    
    # (可选) 保存和加载模型的方法
    def save(self, checkpoint_path):
        pass

    def load(self, checkpoint_path):
        pass
```

### 步骤2: 在`main.py`中替换算法

1. **导入你的算法**:

   ```python
   # 在 main.py 的开头
   # from ippo import PPOAgent  <-- 注释或删除这行
   from my_maddpg import YourAlgorithm  # <-- 导入你的算法类
   ```

2. **实例化你的算法**:
   在`main`函数中，找到智能体初始化的代码块，将其替换为你的算法类。

   ```python
   # 将:
   ppo_agents = {agent.id: PPOAgent(...) for agent in env.agents}
   # 替换为:
   ppo_agents = {agent.id: YourAlgorithm(...) for agent in env.agents}
   ```

完成以上两步后，运行 `main.py` 即可开始使用你的新算法进行训练。

## 6. 环境详解

### 6.1 观测空间 (57维向量)

| 组成部分           | 维度 | 细节                                                     |
| :----------------- | :--- | :------------------------------------------------------- |
| **自身状态**       | 6    | `[x, y, vx, vy, cos(h), sin(h)]` (均已归一化)            |
| **最近目标信息**   | 9    | 3个最近目标的 `[rel_x, rel_y, is_detected]`              |
| **其他智能体状态** | 20   | 5个其他智能体的 `[rel_x, rel_y, vx, vy]`                 |
| **局部探索状态**   | 9    | 周围3×3网格的探索情况 `[0或1]`                           |
| **协同与优先级**   | 13   | 新增信息，包括目标优先级、自身协同角色、共享目标信息等。 |

### 6.2 动作空间

-   **离散空间**: 11个动作，包括不同程度的转向、加减速和停止。
-   **连续空间**: 2维向量 `[速度比例, 转向比例]`。

### 6.3 奖励函数设计哲学

`config.py`中的奖励权重都经过了精心设计，以解决多智能体训练中的特定问题。理解这些奖励的意图，可以帮助你更好地进行实验。

-   `REWARD_DETECT`: **稀疏的核心奖励**。完成最终任务时给予。
-   `REWARD_APPROACHING_TARGET`: **密集的目标引导奖励**。解决奖励稀疏问题，引导智能体朝正确方向移动。
-   `REWARD_BOUNDARY_TARGET`: **专项奖励**。鼓励探测和发现靠近地图边界的“边缘”目标。
-   `REWARD_HIGH_SPEED_TARGET`: **专项奖励**。激励智能体学习追捕和拦截高速移动的特殊目标。
-   `REWARD_EARLY_DETECTION`: **效率奖励**。对在任务早期（例如前20%的时间内）就发现目标的行为给予额外奖励。
-   `REWARD_COORDINATION`: **协同奖励**。当多个智能体共同完成检测时给予，鼓励协作行为。
-   `REWARD_EXPLORE` / `REWARD_EFFICIENT_SEARCH`: **探索激励**。鼓励智能体探索未知区域，提高搜索覆盖率。
-   `REWARD_COLLISION` / `REWARD_OUT_OF_BOUNDS`: **安全惩罚**。权重较高，强制智能体学习避障和区域控制。


## 7. 配置指南 (`config.py`)

`config.py`是所有可调参数的中央枢纽。

- **`ENABLE_RENDERING`**: 是否开启Pygame可视化。**训练时务必设为`False`**以获得数十倍的速度提升。

- **`NUM_UAVS`, `NUM_USVS`**: 控制编队中的智能体数量。

- **IPPO强化学习超参数**:

  -   `UPDATE_TIMESTEPS`: 控制智能体进行一次学习前要收集多少步的经验。
  -   `K_EPOCHS`: PPO算法在一次学习中迭代训练的次数。
  -   `LR_ACTOR`, `LR_CRITIC`: Actor和Critic网络的学习率。

  > **注意**: 这些超参数是为基线IPPO算法调优的，不同的算法可能需要不同的设置。

- **高级奖励函数权重**: 你可以在这里调整上一节中所有奖励组件的权重，包括新增的 `REWARD_BOUNDARY_TARGET`, `REWARD_HIGH_SPEED_TARGET` 等，以测试不同的策略引导效果。

- **课程学习参数**: `main.py`中的`adjust_difficulty`函数会动态调整与目标生成相关的环境参数，你可以在该函数或`environment.py`的`_spawn_targets`方法中调整课程学习的具体逻辑。