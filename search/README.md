# 无人机艇协同搜索强化学习优化框架 (Enhanced)

## 1. 框架定位

这是一个为"无人机艇协同搜索陌生区域水面目标控制算法研究"竞赛场景设计的、经过深度优化的**强化学习框架**。

本框架融合了**传统算法的领域知识**与**强化学习的自适应能力**，实现了以下核心优化：
- **边界目标增强检测**：针对比赛中关键的边界目标进行专项检测优化
- **智能体协同机制**：实现角色分配、信息共享的多智能体协作
- **渐进式课程学习**：从简单场景逐步提升到实际比赛难度
- **区域探索平衡**：解决智能体聚集问题，鼓励全区域搜索
- **目标优先级感知**：智能体可识别高价值目标（高速、边界目标）

开发者可以轻松地在此框架上：
-   **测试新算法**: 替换`ippo.py`为自己的算法实现
-   **对比性能**: 与优化后的基线进行性能比较
-   **实验新策略**: 调整奖励函数、观测空间或环境配置
-   **验证优化效果**: 使用 `test_optimizations.py` 独立验证各项功能

## 2. 快速入门

### 2.1 安装依赖
```bash
pip install -r requirements.txt
```

### 2.2 运行训练
```bash
python main.py
```
默认配置已设为无渲染的高速训练模式。若需可视化，请修改`config.py`。

### 2.3 (可选) 运行优化项验证测试
```bash
python test_optimizations.py
```
该脚本可以独立测试和验证边界检测、协同机制、奖励系统等各项优化功能。

## 3. 项目文件结构解析

| 文件名 | 主要职责 |
| :--- | :--- |
| `main.py` | **主程序入口**。负责组织训练循环，调用环境和算法，并实现**课程学习**策略。 |
| `environment.py` | **核心仿真环境**。基于Gymnasium构建，定义状态、动作、奖励，并实现了**高级检测、协同与奖励逻辑**。 |
| `agents.py` | **智能体物理与协同模型**。定义了UAV、USV和Target的属性、基础移动以及**智能体间的协同交互机制**。 |
| `ippo.py` | **基线算法实现**。提供了一个可运行的IPPO算法作为性能参考。 |
| `config.py` | **中央配置文件**。所有超参数、**高级奖励权重**和物理参数都在此定义。 |
| `evaluator.py` | **性能评估器**。用于计算和输出标准化的性能指标（如成功率、覆盖率）。 |
| `test_optimizations.py` | **新增的验证脚本**。用于独立测试和验证边界检测、协同、奖励系统等各项优化功能。 |
| `README.md` | **开发文档** (本文)。 |
| `requirements.txt` | **项目依赖库**。 |


## 4. 核心特性与优化

此框架融合了传统算法的领域知识与强化学习的自适应能力，实现了以下关键优化：

### 4.1 边界目标增强检测

参考传统搜索算法的优化策略，针对比赛中的关键场景进行专项优化：
- **边界目标探测增强**: 距离边界<200米的目标探测范围增强50%
- **高速目标专项处理**: >10节目标探测范围增强20%，FOV扩大30%
- **动态探测参数**: 根据目标类型自动调整探测灵敏度

### 4.2 智能体协同机制

实现了完整的多智能体协作框架：
- **动态角色分配**: 智能体可扮演searcher（搜索者）、tracker（跟踪者）、interceptor（拦截者）
- **信息共享网络**: 通信范围内智能体共享目标信息，避免重复搜索
- **协同奖励机制**: 多智能体协同检测获得额外奖励（100分）

### 4.3 渐进式课程学习

基于比赛特点设计的三阶段训练策略：
- **初期阶段（Episode 1-100）**: 80%边界目标，3-8节低速，强化基础搜索能力
- **中期阶段（Episode 100-300）**: 50%边界目标，5-12节中速，平衡训练
- **后期阶段（Episode 300+）**: 60%边界目标，8-15节高速，模拟实际比赛

### 4.4 区域探索平衡优化

解决智能体聚集在起始位置的问题：
- **距离起点奖励**: 根据距离起始位置给予渐进式奖励（最高3分）
- **探索效率奖励**: 优先奖励探索其他智能体未访问区域（20分）
- **覆盖率激励**: 定期根据区域覆盖比例给予奖励

### 4.5 扩展的观测空间与目标优先级

**58维观测空间**包含：
- **基础状态**（6维）: 位置、速度、航向信息
- **目标信息**（15维）: 3个最近目标的位置、速度、边界优先级
- **协同信息**（4维）: 当前角色、共享目标数量
- **距离信息**（1维）: 距离起点的归一化距离
- **目标优先级**（3维）: 融合速度和边界位置的目标重要性评分

### 4.6 专项奖励设计

基于比赛目标设计的多层次奖励系统：
- **边界目标奖励**: 300分（比基础检测奖励高50%）
- **高速目标奖励**: 250分（鼓励处理难点目标）
- **早期发现奖励**: 150分（5分钟内发现给予额外奖励）
- **协同检测奖励**: 100分（多智能体协同探测）
- **距离探索奖励**: 3分（鼓励离开起点，平衡搜索区域）

## 5. 如何扩展：实现并测试你的新算法

本框架的核心设计思想是易于扩展。只需遵循以下两步，即可将你的算法集成到框架中。

### 步骤1: 实现算法接口

你需要创建一个新的Python文件（例如 `my_maddpg.py`），并在其中实现一个与`PPOAgent`有相同接口的类。

**算法接口规范**:
```python
class YourAlgorithm:
    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_type):
        """
        构造函数。即使你的算法不需要所有参数，也请保持签名一致以便于替换。
        - state_dim: 状态空间的维度 (当前为57)
        - action_dim: 动作空间的维度 (离散:11, 连续:2)
        - action_type: 'discrete' 或 'continuous'
        """
        # ... 你的初始化代码 ...
        pass

    def select_action(self, state):
        """
        根据当前状态选择一个动作。
        - state: 一个np.array，代表单个智能体的观测值。
        - return: 返回一个动作。对于离散空间，返回一个int；对于连续空间，返回一个np.array。
        """
        # ... 你的动作选择逻辑 ...
        pass

    def update(self):
        """
        执行一次策略更新（训练）。
        框架会在每个 UPDATE_TIMESTEPS 调用此方法。
        你需要在此方法内管理你的经验回放缓冲区。
        """
        # ... 你的训练逻辑 ...
        pass
    
    # (可选) 保存和加载模型的方法
    def save(self, checkpoint_path):
        pass

    def load(self, checkpoint_path):
        pass
```

### 步骤2: 在`main.py`中替换算法

1.  **导入你的算法**:
    ```python
    # 在 main.py 的开头
    # from ippo import PPOAgent  <-- 注释或删除这行
    from my_maddpg import YourAlgorithm  # <-- 导入你的算法类
    ```

2.  **实例化你的算法**:
    在`main`函数中，找到智能体初始化的代码块，将其替换为你的算法类。
    ```python
    # 将:
    ppo_agents = {agent.id: PPOAgent(...) for agent in env.agents}
    # 替换为:
    ppo_agents = {agent.id: YourAlgorithm(...) for agent in env.agents}
    ```

完成以上两步后，运行 `main.py` 即可开始使用你的新算法进行训练。

## 6. 环境详解

### 6.1 观测空间 (58维向量)

| 组成部分 | 维度 | 细节 |
| :--- | :--- | :--- |
| **自身状态** | 6 | `[x, y, vx, vy, cos(h), sin(h)]` (均已归一化) |
| **最近目标信息** | 15 | 3个最近目标的 `[rel_x, rel_y, is_detected, speed, boundary_priority]` |
| **其他智能体状态** | 20 | 5个其他智能体的 `[rel_x, rel_y, vx, vy]` |
| **局部探索状态** | 9 | 周围3×3网格的探索情况 `[0或1]` |
| **目标优先级** | 3 | 3个最近目标的综合优先级评分 `[0-1]` |
| **协同信息** | 4 | `[is_searcher, is_tracker, is_interceptor, shared_targets_count]` |
| **距离信息** | 1 | 距离起始位置的归一化距离 `[0-1]` |

### 6.2 动作空间

-   **离散空间**: 11个动作，包括不同程度的转向、加减速和停止。
-   **连续空间**: 2维向量 `[速度比例, 转向比例]`。

### 6.3 奖励函数设计哲学

`config.py`中的奖励权重经过精心设计，融合了传统算法的领域知识：

**核心检测奖励**:
-   `REWARD_DETECT`: 200分 - 基础检测奖励
-   `REWARD_BOUNDARY_TARGET`: 300分 - 边界目标额外奖励（<200米边界）
-   `REWARD_HIGH_SPEED_TARGET`: 250分 - 高速目标额外奖励（>10节）
-   `REWARD_EARLY_DETECTION`: 150分 - 早期发现奖励（<5分钟）
-   `REWARD_COORDINATION_DETECT`: 100分 - 协同检测奖励

**探索与搜索奖励**:
-   `REWARD_EXPLORE`: 1分 - 探索新网格奖励
-   `REWARD_EFFICIENT_SEARCH`: 20分 - 高效搜索奖励（避免重复搜索）
-   `REWARD_DISTANCE_EXPLORATION`: 3分 - 距离起点探索奖励
-   `REWARD_AREA_COVERAGE`: 10分 - 区域覆盖奖励
-   `REWARD_TARGET_TRACKING`: 5分 - 目标跟踪奖励

**行为引导奖励**:
-   `REWARD_APPROACHING_TARGET`: 2分 - 接近目标奖励
-   `REWARD_MOVEMENT`: 0.1分 - 移动激励奖励

**安全与约束惩罚**:
-   `REWARD_COLLISION`: -30分 - 碰撞惩罚
-   `REWARD_OUT_OF_BOUNDS`: -15分 - 出界惩罚
-   `REWARD_TIME_STEP`: -0.01分 - 时间惩罚
-   `REWARD_INEFFICIENT_SEARCH`: -5分 - 重复搜索惩罚


## 7. 配置指南 (`config.py`)

`config.py`是所有参数的中央控制器，包含以下主要配置：

**训练控制参数**:
-   **`ENABLE_RENDERING`**: 训练时务必设为`False`以获得数十倍速度提升
-   **`NUM_UAVS`, `NUM_USVS`**: 控制编队规模（默认2架无人机，4艘无人艇）
-   **`UPDATE_TIMESTEPS`**: 控制学习频率（默认4000步）

**IPPO算法超参数**:
-   **`LR_ACTOR`, `LR_CRITIC`**: Actor和Critic网络学习率
-   **`K_EPOCHS`**: PPO训练迭代次数
-   **`GAMMA`**: 奖励折扣因子

**奖励系统权重** (已优化):
-   所有奖励权重已根据比赛特点精心调整
-   重点强化边界目标（300分）和高速目标（250分）检测
-   平衡探索激励与任务完成奖励

**课程学习参数** (动态调整):
-   **`TARGET_BOUNDARY_PROB`**: 边界目标生成概率（训练中动态调整）
-   **`TARGET_SPEED_RANGE`**: 目标速度范围（渐进式增加难度）
-   **`TARGET_GENERATION_RATE`**: 目标生成频率（防止目标过多）

## 8. 性能验证与测试

### 8.1 使用验证脚本
```bash
python test_optimizations.py
```

该脚本会自动验证：
- ✅ 边界目标检测增强效果
- ✅ 奖励系统正确性
- ✅ 观测空间完整性（58维）
- ✅ 协同机制角色分配
- ✅ 距离探索奖励机制
- ✅ 课程学习参数调整

### 8.2 性能指标监控

训练过程中重点关注：
- **目标检测成功率**: 特别是边界和高速目标
- **区域覆盖率**: 智能体是否均匀分布搜索
- **协同效率**: 多智能体协作检测频率
- **平均奖励趋势**: 应呈上升趋势且收敛

### 8.3 与基线对比

建议对比指标：
- 边界目标检测率提升
- 高速目标处理能力
- 整体搜索效率
- 训练收敛速度

## 9. 优化总结

本框架实现了传统算法与强化学习的深度融合：

1. **领域知识融入**: 边界优先、高速目标处理等比赛关键点
2. **协同机制优化**: 角色分配、信息共享的多智能体协作
3. **渐进式训练**: 课程学习确保训练稳定性和最终性能
4. **区域平衡**: 解决智能体聚集问题，实现全区域高效搜索
5. **智能感知**: 58维观测空间包含目标优先级和协同状态

这些优化预期将显著提升在"无人机艇协同搜索陌生区域水面目标"竞赛中的表现。
