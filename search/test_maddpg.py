# test_maddpg.py - ä¼˜åŒ–ç‰ˆMADDPGæ€§èƒ½æµ‹è¯•è„šæœ¬
import numpy as np
import torch
from environment import UsvUavEnv
from maddpg import MADDPG
import config
import time
import matplotlib.pyplot as plt

def test_maddpg_optimized():
    """æµ‹è¯•ä¼˜åŒ–ç‰ˆMADDPGçš„æ€§èƒ½æ”¹è¿›"""
    print("=== ä¼˜åŒ–ç‰ˆMADDPGæ€§èƒ½æµ‹è¯• ===")
    
    # æ£€æŸ¥PyTorchå’ŒCUDA
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = UsvUavEnv(action_type='continuous')
    obs, _ = env.reset()
    
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    num_agents = len(env.agents)
    
    print(f"ç¯å¢ƒå‚æ•°: {num_agents}ä¸ªæ™ºèƒ½ä½“, çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆMADDPG
    maddpg = MADDPG(
        num_agents=num_agents,
        state_dim=state_dim,
        action_dim=action_dim,
        lr_actor=config.LR_ACTOR,      # ä½¿ç”¨æ–°çš„å­¦ä¹ ç‡
        lr_critic=config.LR_CRITIC,
        gamma=config.GAMMA,
        tau=config.MADDPG_TAU,         # ä½¿ç”¨æ–°çš„è½¯æ›´æ–°å‚æ•°
        max_action=1.0,
        buffer_size=config.MADDPG_BUFFER_SIZE,
        device=device,
        weight_decay=config.WEIGHT_DECAY  # ä½¿ç”¨æƒé‡è¡°å‡
    )
    
    # è®¾ç½®ä¼˜åŒ–åçš„å‚æ•°
    maddpg.batch_size = config.MADDPG_BATCH_SIZE
    maddpg.update_freq = config.MADDPG_UPDATE_FREQ
    
    print("ä¼˜åŒ–ç‰ˆMADDPGåˆå§‹åŒ–æˆåŠŸ!")
    print(f"è®­ç»ƒå‚æ•°: LR_A={config.LR_ACTOR}, LR_C={config.LR_CRITIC}, TAU={config.MADDPG_TAU}")
    print(f"æ‰¹æ¬¡å¤§å°: {maddpg.batch_size}, æ›´æ–°é¢‘ç‡: {maddpg.update_freq}")
    
    # è¿è¡Œæµ‹è¯•è®­ç»ƒ
    episode_rewards = []
    episode_steps = []
    episode_times = []
    detection_rates = []
    termination_reasons = []
    
    print("\nå¼€å§‹ä¼˜åŒ–ç‰ˆè®­ç»ƒæµ‹è¯•(20ä¸ªepisode)...")
    for episode in range(20):
        start_time = time.time()
        obs, _ = env.reset()
        maddpg.reset_noise()
        
        episode_reward = 0
        steps = 0
        
        while steps < 3000:  # æœ€å¤§3000æ­¥ï¼Œå¯¹åº”5åˆ†é’Ÿ
            # è·å–çŠ¶æ€
            states = []
            agent_ids = list(obs.keys())
            for agent_id in agent_ids:
                states.append(obs[agent_id])
            states = np.array(states)
            
            # é€‰æ‹©åŠ¨ä½œ
            actions = maddpg.select_actions(states, add_noise=True)
            
            # ç¯å¢ƒäº¤äº’
            env_actions = {}
            for i, agent_id in enumerate(agent_ids):
                env_actions[agent_id] = actions[i]
            
            next_obs, rewards, terminated, truncated, info = env.step(env_actions)
            
            # å­˜å‚¨ç»éªŒå’Œæ›´æ–°
            next_states = []
            reward_list = []
            done_list = []
            
            for agent_id in agent_ids:
                next_states.append(next_obs[agent_id])
                reward_list.append(rewards[agent_id])
                done_list.append(terminated or truncated)
            
            next_states = np.array(next_states)
            reward_list = np.array(reward_list)
            done_list = np.array(done_list)
            
            maddpg.store_transition(states, actions, reward_list, next_states, done_list)
            
            episode_reward += sum(reward_list)
            steps += 1
            
            # ç½‘ç»œæ›´æ–°ï¼ˆä½¿ç”¨æ–°çš„æ›´æ–°é¢‘ç‡ï¼‰
            if len(maddpg.replay_buffer) >= maddpg.batch_size:
                maddpg.update()
            
            obs = next_obs
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        episode_rewards.append(episode_reward)
        episode_steps.append(steps)
        episode_times.append(episode_time)
        
        # è·å–ç»ˆæ­¢ä¿¡æ¯
        detection_rate = info.get('detection_rate', 0)
        detected_count = info.get('detected_count', 0)
        total_spawned = info.get('total_spawned', 0)
        termination_reason = info.get('termination_reason', 'unknown')
        
        detection_rates.append(detection_rate)
        termination_reasons.append(termination_reason)
        
        print(f"Episode {episode+1}: å¥–åŠ±={episode_reward:.1f}, æ­¥æ•°={steps}, "
              f"æ£€æµ‹ç‡={detection_rate:.1%} ({detected_count}/{total_spawned}), "
              f"ç»ˆæ­¢åŸå› ={termination_reason}, ç”¨æ—¶={episode_time:.1f}s")
    
    # åˆ†æç»“æœ
    avg_reward = np.mean(episode_rewards)
    avg_steps = np.mean(episode_steps)
    avg_time = np.mean(episode_times)
    avg_detection_rate = np.mean(detection_rates)
    
    print(f"\n=== ä¼˜åŒ–ç‰ˆMADDPGæµ‹è¯•ç»“æœ ===")
    print(f"å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
    print(f"å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
    print(f"å¹³å‡ç”¨æ—¶: {avg_time:.1f}ç§’")
    print(f"å¹³å‡æ£€æµ‹ç‡: {avg_detection_rate:.1%}")
    print(f"ç¼“å†²åŒºå¤§å°: {len(maddpg.replay_buffer)}")
    
    # åˆ†æç»ˆæ­¢åŸå› 
    from collections import Counter
    reason_counts = Counter(termination_reasons)
    print(f"ç»ˆæ­¢åŸå› ç»Ÿè®¡: {dict(reason_counts)}")
    
    # æ£€æŸ¥ç½‘ç»œå­¦ä¹ æƒ…å†µ
    if len(maddpg.agents[0].actor_loss_history) > 10:
        recent_actor_losses = [np.mean(agent.actor_loss_history[-10:]) 
                              for agent in maddpg.agents if len(agent.actor_loss_history) >= 10]
        recent_critic_losses = [np.mean(agent.critic_loss_history[-10:]) 
                               for agent in maddpg.agents if len(agent.critic_loss_history) >= 10]
        
        if recent_actor_losses and recent_critic_losses:
            avg_actor_loss = np.mean(recent_actor_losses)
            avg_critic_loss = np.mean(recent_critic_losses)
            print(f"æœ€è¿‘ç½‘ç»œæŸå¤±: Actor={avg_actor_loss:.4f}, Critic={avg_critic_loss:.1f}")
    
    # å­¦ä¹ è¶‹åŠ¿åˆ†æ
    if len(episode_rewards) >= 10:
        first_half = np.mean(episode_rewards[:10])
        second_half = np.mean(episode_rewards[-10:])
        improvement = (second_half - first_half) / abs(first_half) * 100 if first_half != 0 else 0
        print(f"å­¦ä¹ æ”¹è¿›: {improvement:.1f}% (ä»{first_half:.1f}åˆ°{second_half:.1f})")
    
    return {
        'avg_reward': avg_reward,
        'avg_detection_rate': avg_detection_rate,
        'avg_steps': avg_steps,
        'avg_time': avg_time,
        'improvement': improvement,
        'buffer_size': len(maddpg.replay_buffer)
    }

def main():
    """è¿è¡Œä¼˜åŒ–ç‰ˆMADDPGæµ‹è¯•"""
    print("å¼€å§‹MADDPGä¼˜åŒ–ç‰ˆéªŒè¯æµ‹è¯•\n")
    
    try:
        results = test_maddpg_optimized()
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("MADDPGä¼˜åŒ–ç‰ˆæµ‹è¯•æ€»ç»“")
        print("="*60)
        print("âœ“ å¥–åŠ±ç³»ç»Ÿé‡æ–°å¹³è¡¡: é¿å…æ¢¯åº¦çˆ†ç‚¸")
        print("âœ“ ç½‘ç»œå‚æ•°ä¼˜åŒ–: é™ä½å­¦ä¹ ç‡å’Œæ‰¹æ¬¡å¤§å°")
        print("âœ“ Episodeç»ˆæ­¢æ¡ä»¶: åŠ¨æ€ç»ˆæ­¢æé«˜æ•ˆç‡")
        print("âœ“ æ¢ç´¢ç­–ç•¥ä¼˜åŒ–: ä½ç½®å¤šæ ·æ€§å’Œå™ªå£°è°ƒæ•´")
        print("âœ“ ååŒæœºåˆ¶å¢å¼º: åŒºåŸŸåˆ†å·¥å’ŒååŒå¥–åŠ±")
        
        print(f"\nå…³é”®æ€§èƒ½æŒ‡æ ‡:")
        print(f"  - å¹³å‡æ£€æµ‹ç‡: {results['avg_detection_rate']:.1%}")
        print(f"  - è®­ç»ƒæ•ˆç‡: {results['avg_steps']:.0f}æ­¥/episode")
        print(f"  - å­¦ä¹ æ”¹è¿›: {results.get('improvement', 0):.1f}%")
        print(f"  - ç»éªŒç§¯ç´¯: {results['buffer_size']}æ¡ç»éªŒ")
        
        success_rate = results['avg_detection_rate']
        if success_rate > 0.4:
            print("\nğŸ¯ ä¼˜åŒ–æ•ˆæœæ˜¾è‘—! MADDPGå·²å‡†å¤‡å¥½è¿›è¡Œæ­£å¼è®­ç»ƒã€‚")
        elif success_rate > 0.2:
            print("\nâš¡ ä¼˜åŒ–æœ‰æ•ˆ! å¯ç»§ç»­è°ƒæ•´å‚æ•°ä»¥è¿›ä¸€æ­¥æå‡ã€‚")
        else:
            print("\nğŸ”§ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå»ºè®®æ£€æŸ¥å¥–åŠ±å‡½æ•°å’Œç½‘ç»œç»“æ„ã€‚")
        
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()