# main.py
import pygame
import numpy as np
from environment import UsvUavEnv
from ippo import PPOAgent
import config

def adjust_difficulty(episode):
    """
    è¯¾ç¨‹å­¦ä¹ ï¼šæ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´éš¾åº¦
    """
    if episode < 100:
        # åˆæœŸï¼šæ›´å¤šæ­£å¥–åŠ±ï¼Œå‡å°‘æƒ©ç½šï¼Œé¼“åŠ±æ¢ç´¢
        config.REWARD_TIME_STEP = -0.005
        config.REWARD_EXPLORE = 2.0
        config.REWARD_COLLISION = -10.0
        config.REWARD_OUT_OF_BOUNDS = -5.0
        print(f"    [Curriculum] Easy mode: Enhanced exploration rewards")
    elif episode < 300:
        # ä¸­æœŸï¼šæ ‡å‡†è®¾ç½®
        config.REWARD_TIME_STEP = -0.01
        config.REWARD_EXPLORE = 1.0
        config.REWARD_COLLISION = -20.0
        config.REWARD_OUT_OF_BOUNDS = -10.0
        if episode == 100:
            print(f"    [Curriculum] Standard mode: Balanced rewards")
    else:
        # åæœŸï¼šæ›´ä¸¥æ ¼çš„è¦æ±‚ï¼Œè¿½æ±‚æ•ˆç‡
        config.REWARD_TIME_STEP = -0.02
        config.REWARD_EXPLORE = 0.8
        config.REWARD_COLLISION = -30.0
        config.REWARD_OUT_OF_BOUNDS = -15.0
        if episode == 300:
            print(f"    [Curriculum] Hard mode: Efficiency focused")

def main():
    # é€‰æ‹©åŠ¨ä½œç±»å‹ï¼š'discrete' æˆ– 'continuous'  
    action_type = 'discrete'  # æš‚æ—¶åˆ‡æ¢åˆ°ç¦»æ•£åŠ¨ä½œï¼Œè°ƒè¯•å®Œæˆåå†æ”¹å›è¿ç»­
    env = UsvUavEnv(action_type=action_type)
    
    # åªæœ‰å¯ç”¨æ¸²æŸ“æ—¶æ‰åˆå§‹åŒ–æ—¶é’Ÿ
    if config.ENABLE_RENDERING:
        clock = pygame.time.Clock()
    else:
        clock = None
        print("è®­ç»ƒæ¨¡å¼ï¼šå·²ç¦ç”¨å¯è§†åŒ–æ¸²æŸ“ä»¥æé«˜è®­ç»ƒé€Ÿåº¦")

    # --- 1. IPPOæ™ºèƒ½ä½“åˆå§‹åŒ– ---
    # å°†ppo_agentsçš„åˆå§‹åŒ–å»¶è¿Ÿåˆ°resetä¹‹åï¼Œä»¥ç¡®ä¿èƒ½è·å–åˆ°æ­£ç¡®çš„agentåˆ—è¡¨
    ppo_agents = {}
    is_ppo_agents_initialized = False

    # --- 2. ä¸»è®­ç»ƒå¾ªç¯ ---
    time_step = 0
    num_episodes = 500 # å¢åŠ è®­ç»ƒå›åˆæ•°
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    detection_stats = []
    
    for episode in range(num_episodes):
        # åº”ç”¨è¯¾ç¨‹å­¦ä¹ 
        adjust_difficulty(episode)
        
        print(f"--- Episode {episode + 1} ---")
        observations, info = env.reset()

        # ä»…åœ¨ç¬¬ä¸€ä¸ªå›åˆåˆå§‹åŒ–PPOæ™ºèƒ½ä½“
        if not is_ppo_agents_initialized:
            state_dim = env.observation_space.shape[0]
            if action_type == 'continuous':
                action_dim = env.action_space.shape[0]
            else:
                action_dim = env.action_space.n
            
            ppo_agents = {agent.id: PPOAgent(state_dim, action_dim, config.LR_ACTOR, 
                                             config.LR_CRITIC, config.GAMMA, 
                                             config.K_EPOCHS, config.EPS_CLIP, action_type)
                          for agent in env.agents}
            is_ppo_agents_initialized = True

        terminated = False
        truncated = False
        running = True
        current_episode_reward = 0
        episode_detected_count = 0
        episode_steps = 0
        
        while running and not terminated and not truncated:
            time_step += 1
            episode_steps += 1
            
            # --- çª—å£äº‹ä»¶å¤„ç†ï¼ˆä»…åœ¨æ¸²æŸ“æ¨¡å¼ä¸‹ï¼‰---
            if config.ENABLE_RENDERING:
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        running = False

            # --- 3. ç®—æ³•å†³ç­–ä¸æ•°æ®æ”¶é›† ---
            actions = {}
            for agent_id, obs in observations.items():
                # ä»å¯¹åº”çš„PPOAgentè·å–åŠ¨ä½œ
                action = ppo_agents[agent_id].select_action(obs)
                actions[agent_id] = action
                
                # è°ƒè¯•ï¼šå¶å°”æ‰“å°åŠ¨ä½œä¿¡æ¯
                if time_step % 5000 == 0 and agent_id == 'usv_0':
                    print(f"    DEBUG: {agent_id} action = {action} (type: {type(action)})")
                    if hasattr(env.agents[0], 'pos'):
                        print(f"    DEBUG: {agent_id} position = {env.agents[0].pos}")
                        print(f"    DEBUG: {agent_id} velocity = {env.agents[0].velocity}")
                        print(f"    DEBUG: Action type = {action_type}")

            # --- ç¯å¢ƒäº¤äº’ ---
            next_observations, rewards, terminated, truncated, info = env.step(actions)

            # --- 4. å°†ç»éªŒå­˜å…¥æ¯ä¸ªæ™ºèƒ½ä½“çš„Buffer ---
            for agent_id, ppo_agent in ppo_agents.items():
                ppo_agent.buffer.rewards.append(rewards[agent_id])
                # å¯¹IPPOæ¥è¯´ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½å°†å…¨å±€çš„doneä¿¡å·ä½œä¸ºè‡ªå·±çš„ç»ˆæ­¢ä¿¡å·
                ppo_agent.buffer.is_terminals.append(terminated or truncated)

            observations = next_observations
            current_episode_reward += sum(rewards.values())
            
            # ç»Ÿè®¡æ£€æµ‹äº‹ä»¶
            if 'detected_events' in info:
                episode_detected_count += len([event for event in info['detected_events'] 
                                             if 'successfully detected' in event])

            # --- 5. æ¨¡å‹æ›´æ–° ---
            if time_step % config.UPDATE_TIMESTEPS == 0:
                print(f"    UPDATING at time_step {time_step}...")
                for agent_id, ppo_agent in ppo_agents.items():
                    ppo_agent.update()
                print(f"    UPDATE finished.")
            
            # --- è®­ç»ƒç›‘æ§ï¼ˆè¯¦ç»†ç‰ˆï¼‰---
            if config.ENABLE_RENDERING:
                env.render()
                clock.tick(config.TARGET_FPS)
            elif time_step % 1000 == 0:  # æ— æ¸²æŸ“æ¨¡å¼ä¸‹çš„è¯¦ç»†ç›‘æ§
                avg_reward = current_episode_reward / episode_steps if episode_steps > 0 else 0
                print(f"    Step {time_step}, Episode Steps: {episode_steps}, Avg Reward: {avg_reward:.3f}, Episode Reward: {current_episode_reward:.1f}")
                
                # æ£€æŸ¥å½“å‰æ£€æµ‹çŠ¶æ€
                active_detections = sum(1 for target in env.targets 
                                      if hasattr(target, 'detection_completed') and target.detection_completed)
                total_targets = len(env.targets)
                print(f"    Targets: {active_detections}/{total_targets} detected, Episode total: {episode_detected_count}")
                
                # æ˜¾ç¤ºæ™ºèƒ½ä½“çŠ¶æ€ï¼ˆåŒ…å«æ‰€æœ‰æ™ºèƒ½ä½“ï¼‰
                all_agents = [(agent.id, agent.pos) for agent in env.agents]
                print(f"    All agents: {[(aid, f'({pos[0]:.0f},{pos[1]:.0f})') for aid, pos in all_agents]}")
                print(f"    Total agents: {len(env.agents)} (should be {config.TOTAL_AGENTS})")
                
                # æ˜¾ç¤ºå½“å‰å¥–åŠ±ç»„æˆï¼ˆè°ƒè¯•ç”¨ï¼‰
                recent_rewards = {aid: rewards.get(aid, 0) for aid in list(rewards.keys())[:2]}
                print(f"    Recent rewards sample: {recent_rewards}")
                print(f"    Current curriculum: Time penalty={config.REWARD_TIME_STEP}, Explore reward={config.REWARD_EXPLORE}")
                print(f"    Episode time: {env.current_time:.1f}s")
                print("-" * 60)
                
                # æ—©åœæ£€æŸ¥ï¼šå¦‚æœæ­¥æ•°è¿‡å¤šï¼Œå¼ºåˆ¶ç»“æŸå›åˆ
                if episode_steps > 50000:  # 50000æ­¥åå¼ºåˆ¶ç»“æŸ
                    print(f"    [EARLY STOP] Episode too long, forcing termination")
                    break

        # å›åˆç»“æŸç»Ÿè®¡
        episode_rewards.append(current_episode_reward)
        detection_stats.append(episode_detected_count)
        
        # å›åˆæ‘˜è¦
        print(f"Episode {episode + 1} Summary:")
        print(f"  Total Reward: {current_episode_reward:.1f}")
        print(f"  Targets Detected: {episode_detected_count}")
        print(f"  Steps: {episode_steps}")
        
        # æ¯10å›åˆæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 10 == 0:
            recent_rewards = episode_rewards[-10:]
            recent_detections = detection_stats[-10:]
            avg_reward = sum(recent_rewards) / len(recent_rewards)
            avg_detections = sum(recent_detections) / len(recent_detections)
            
            print(f"\n{'='*50}")
            print(f"TRAINING PROGRESS - Episode {episode + 1}")
            print(f"{'='*50}")
            print(f"Last 10 episodes average reward: {avg_reward:.1f}")
            print(f"Last 10 episodes average detections: {avg_detections:.1f}")
            print(f"Best episode reward so far: {max(episode_rewards):.1f}")
            print(f"Best detection count so far: {max(detection_stats)}")
            
            # å­¦ä¹ è¶‹åŠ¿åˆ†æ
            if len(episode_rewards) >= 20:
                first_half = sum(episode_rewards[-20:-10]) / 10
                second_half = sum(episode_rewards[-10:]) / 10
                trend = "â†—ï¸ Improving" if second_half > first_half else "â†˜ï¸ Declining" if second_half < first_half else "â†’ Stable"
                print(f"Learning trend: {trend} ({second_half:.1f} vs {first_half:.1f})")
            
            print(f"{'='*50}\n")
    
    # è®­ç»ƒå®Œæˆæ‘˜è¦
    print("\n" + "ğŸ¯ TRAINING COMPLETED ğŸ¯".center(60, "="))
    print(f"Total episodes: {num_episodes}")
    print(f"Best episode reward: {max(episode_rewards):.1f}")
    print(f"Final 10 episodes average: {sum(episode_rewards[-10:]) / 10:.1f}")
    print(f"Best detection performance: {max(detection_stats)} targets")
    print(f"Average detections (final 10): {sum(detection_stats[-10:]) / 10:.1f}")
    print("=" * 60)
    
    env.close()
    print("Simulation finished.")

if __name__ == "__main__":
    main()