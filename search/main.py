# main.py
import pygame
import numpy as np
from environment import UsvUavEnv
from maddpg import MADDPG, PPOAgent  # ä½¿ç”¨MADDPGæ›¿æ¢IPPO
import config
import torch

def adjust_difficulty(episode):
    """
    æ”¹è¿›çš„è¯¾ç¨‹å­¦ä¹ ï¼šæ ¹æ®æ¯”èµ›ç‰¹ç‚¹åŠ¨æ€è°ƒæ•´éš¾åº¦
    é‡æ–°å¹³è¡¡å¥–åŠ±æƒé‡ä»¥é¿å…æ¢¯åº¦çˆ†ç‚¸
    """
    if episode < 100:
        # åˆæœŸï¼šè¾¹ç•Œç›®æ ‡ä¸ºä¸»ï¼Œé€Ÿåº¦è¾ƒä½ï¼Œä¾¿äºå­¦ä¹ åŸºç¡€æ¢æµ‹
        config.REWARD_TIME_STEP = -0.05  # é™ä½æ—¶é—´æƒ©ç½š
        config.REWARD_EXPLORE = 1.0      # å¢å¼ºæ¢ç´¢å¥–åŠ±
        config.REWARD_COLLISION = -5.0   # é™ä½ç¢°æ’æƒ©ç½š
        config.REWARD_OUT_OF_BOUNDS = -2.0 # é™ä½å‡ºç•Œæƒ©ç½š
        
        # ç›®æ ‡ç”Ÿæˆå‚æ•°ï¼ˆæ–°å¢ï¼‰
        config.TARGET_BOUNDARY_PROB = 0.8  # 80%æ¦‚ç‡ç”Ÿæˆè¾¹ç•Œç›®æ ‡
        config.TARGET_SPEED_RANGE = (3, 8)  # 3-8èŠ‚é€Ÿåº¦èŒƒå›´
        config.TARGET_GENERATION_RATE = 0.01  # è¿›ä¸€æ­¥é™ä½ç”Ÿæˆé¢‘ç‡ï¼ˆä»0.02é™åˆ°0.01ï¼‰
        
        print(f"    [Curriculum] Easy mode: è¾¹ç•Œç›®æ ‡ä¸»å¯¼, ä½é€Ÿç›®æ ‡, æ¢ç´¢å¥–åŠ±å¢å¼º")
        
    elif episode < 300:
        # ä¸­æœŸï¼šæ··åˆç›®æ ‡ç±»å‹ï¼Œé€æ­¥å¢åŠ éš¾åº¦
        config.REWARD_TIME_STEP = -0.1   # æ ‡å‡†æ—¶é—´æƒ©ç½š
        config.REWARD_EXPLORE = 0.5      # æ ‡å‡†æ¢ç´¢å¥–åŠ±
        config.REWARD_COLLISION = -8.0   # ä¸­ç­‰ç¢°æ’æƒ©ç½š
        config.REWARD_OUT_OF_BOUNDS = -4.0 # ä¸­ç­‰å‡ºç•Œæƒ©ç½š
        
        # ç›®æ ‡ç”Ÿæˆå‚æ•°
        config.TARGET_BOUNDARY_PROB = 0.5  # 50%æ¦‚ç‡è¾¹ç•Œç›®æ ‡
        config.TARGET_SPEED_RANGE = (5, 12)  # 5-12èŠ‚é€Ÿåº¦èŒƒå›´
        config.TARGET_GENERATION_RATE = 0.015  # é€‚ä¸­ç”Ÿæˆé¢‘ç‡
        
        if episode == 100:
            print(f"    [Curriculum] Medium mode: ç›®æ ‡ç±»å‹å¹³è¡¡, é€Ÿåº¦æå‡")
            
    else:
        # åæœŸï¼šé«˜é€Ÿç›®æ ‡ä¸ºä¸»ï¼Œæ¨¡æ‹Ÿå®é™…æ¯”èµ›æ¡ä»¶
        config.REWARD_TIME_STEP = -0.15  # æé«˜æ—¶é—´å‹åŠ›
        config.REWARD_EXPLORE = 0.3      # é™ä½æ¢ç´¢å¥–åŠ±ï¼Œé‡è§†æ•ˆç‡
        config.REWARD_COLLISION = -10.0  # æ¢å¤æ ‡å‡†ç¢°æ’æƒ©ç½š
        config.REWARD_OUT_OF_BOUNDS = -5.0 # æ¢å¤æ ‡å‡†å‡ºç•Œæƒ©ç½š
        
        # ç›®æ ‡ç”Ÿæˆå‚æ•°
        config.TARGET_BOUNDARY_PROB = 0.6  # 60%æ¦‚ç‡è¾¹ç•Œç›®æ ‡ï¼ˆå®é™…æ¯”èµ›ç‰¹ç‚¹ï¼‰
        config.TARGET_SPEED_RANGE = (8, 15)  # 8-15èŠ‚é«˜é€Ÿç›®æ ‡
        config.TARGET_GENERATION_RATE = 0.02  # æ ‡å‡†ç”Ÿæˆé¢‘ç‡
        
        if episode == 300:
            print(f"    [Curriculum] Hard mode: é«˜é€Ÿç›®æ ‡ä¸»å¯¼, æ•ˆç‡ä¼˜å…ˆ")

def generate_curriculum_target(env):
    """
    åŸºäºè¯¾ç¨‹å­¦ä¹ å‚æ•°ç”Ÿæˆç›®æ ‡
    """
    import random
    
    # æ ¹æ®è¯¾ç¨‹è®¾ç½®ç¡®å®šç›®æ ‡ç±»å‹
    is_boundary = random.random() < getattr(config, 'TARGET_BOUNDARY_PROB', 0.5)
    speed_range = getattr(config, 'TARGET_SPEED_RANGE', (5, 15))
    
    if is_boundary:
        # è¾¹ç•Œç›®æ ‡ç”Ÿæˆ
        edge = random.choice(['top', 'bottom', 'left', 'right'])
        if edge == 'top':
            pos = [random.uniform(0, config.AREA_WIDTH_METERS), config.AREA_HEIGHT_METERS - 50]
            heading = random.uniform(-np.pi * 0.75, -np.pi * 0.25)  # æœå‘ä¸‹æ–¹
        elif edge == 'bottom':
            pos = [random.uniform(0, config.AREA_WIDTH_METERS), 50]
            heading = random.uniform(np.pi * 0.25, np.pi * 0.75)  # æœå‘ä¸Šæ–¹
        elif edge == 'left':
            pos = [50, random.uniform(0, config.AREA_HEIGHT_METERS)]
            heading = random.uniform(-np.pi * 0.25, np.pi * 0.25)  # æœå‘å³æ–¹
        else:  # right
            pos = [config.AREA_WIDTH_METERS - 50, random.uniform(0, config.AREA_HEIGHT_METERS)]
            heading = random.uniform(np.pi * 0.75, np.pi * 1.25)  # æœå‘å·¦æ–¹
    else:
        # ä¸­å¿ƒåŒºåŸŸç›®æ ‡
        pos = [
            random.uniform(config.AREA_WIDTH_METERS * 0.2, config.AREA_WIDTH_METERS * 0.8),
            random.uniform(config.AREA_HEIGHT_METERS * 0.2, config.AREA_HEIGHT_METERS * 0.8)
        ]
        heading = random.uniform(0, 2 * np.pi)
    
    # è®¾ç½®é€Ÿåº¦
    speed_knots = random.uniform(*speed_range)
    speed_mps = speed_knots * config.KNOTS_TO_MPS
    velocity = [speed_mps * np.cos(heading), speed_mps * np.sin(heading)]
    
    from agents import Target
    target = Target(f"target_{len(env.targets)}", pos, heading)
    target.velocity = np.array(velocity)
    target.spawn_time = env.current_time
    
    return target

def main():
    # é€‰æ‹©åŠ¨ä½œç±»å‹ï¼š'discrete' æˆ– 'continuous'  
    action_type = 'continuous'  # MADDPGä¸“ä¸ºè¿ç»­åŠ¨ä½œè®¾è®¡
    env = UsvUavEnv(action_type=action_type)
    
    # åªæœ‰å¯ç”¨æ¸²æŸ“æ—¶æ‰åˆå§‹åŒ–æ—¶é’Ÿ
    if config.ENABLE_RENDERING:
        clock = pygame.time.Clock()
    else:
        clock = None
        print("MADDPGè®­ç»ƒæ¨¡å¼ï¼šå·²ç¦ç”¨å¯è§†åŒ–æ¸²æŸ“ä»¥æé«˜è®­ç»ƒé€Ÿåº¦")

    # --- 1. MADDPGæ™ºèƒ½ä½“åˆå§‹åŒ– ---  
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    maddpg = None
    is_maddpg_initialized = False

    # --- 2. ä¸»è®­ç»ƒå¾ªç¯ ---
    time_step = 0
    num_episodes = 500 # å¢åŠ è®­ç»ƒå›åˆæ•°
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    detection_stats = []
    
    for episode in range(num_episodes):
        # åº”ç”¨è¯¾ç¨‹å­¦ä¹ 
        adjust_difficulty(episode)
        
        print(f"--- Episode {episode + 1} ---")
        observations, info = env.reset()

        # ä»…åœ¨ç¬¬ä¸€ä¸ªå›åˆåˆå§‹åŒ–MADDPG
        if not is_maddpg_initialized:
            state_dim = env.observation_space.shape[0]
            if action_type == 'continuous':
                action_dim = env.action_space.shape[0]
            else:
                action_dim = env.action_space.n
            
            num_agents = len(env.agents)
            
            # åˆ›å»ºMADDPGå®ä¾‹
            maddpg = MADDPG(
                num_agents=num_agents,
                state_dim=state_dim,
                action_dim=action_dim,
                lr_actor=config.LR_ACTOR,
                lr_critic=config.LR_CRITIC,
                gamma=config.GAMMA,
                tau=config.MADDPG_TAU,
                max_action=1.0,
                buffer_size=config.MADDPG_BUFFER_SIZE,
                device=device
            )
            
            # æ›´æ–°MADDPGçš„è®­ç»ƒå‚æ•°
            maddpg.batch_size = config.MADDPG_BATCH_SIZE
            maddpg.update_freq = config.MADDPG_UPDATE_FREQ
            
            # ä¸ºå…¼å®¹æ€§åˆ›å»ºppo_agentsæ˜ å°„
            ppo_agents = {}
            for i, agent in enumerate(env.agents):
                ppo_agent = PPOAgent(state_dim, action_dim, config.LR_ACTOR, 
                                   config.LR_CRITIC, config.GAMMA, 
                                   config.K_EPOCHS, config.EPS_CLIP, action_type)
                ppo_agent.maddpg_agent = maddpg.agents[i]
                ppo_agent.agent_id = i
                ppo_agents[agent.id] = ppo_agent
                
            is_maddpg_initialized = True
            print(f"MADDPGåˆå§‹åŒ–å®Œæˆ: {num_agents}ä¸ªæ™ºèƒ½ä½“")

        # é‡ç½®å™ªå£°
        maddpg.reset_noise()

        terminated = False
        truncated = False
        running = True
        current_episode_reward = 0
        episode_detected_count = 0
        episode_steps = 0
        
        while running and not terminated and not truncated:
            time_step += 1
            episode_steps += 1
            
            # --- çª—å£äº‹ä»¶å¤„ç†ï¼ˆä»…åœ¨æ¸²æŸ“æ¨¡å¼ä¸‹ï¼‰---
            if config.ENABLE_RENDERING:
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        running = False

            # --- 3. MADDPGç®—æ³•å†³ç­–ä¸æ•°æ®æ”¶é›† ---
            # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹
            states = []
            agent_ids = []
            for agent_id, obs in observations.items():
                states.append(obs)
                agent_ids.append(agent_id)
            states = np.array(states)
            
            # MADDPGé€‰æ‹©åŠ¨ä½œ
            maddpg_actions = maddpg.select_actions(states, add_noise=True)
            
            # æ„å»ºç¯å¢ƒæ‰€éœ€çš„åŠ¨ä½œå­—å…¸
            actions = {}
            for i, agent_id in enumerate(agent_ids):
                actions[agent_id] = maddpg_actions[i]
                
            # è°ƒè¯•ï¼šå¶å°”æ‰“å°åŠ¨ä½œä¿¡æ¯
            if time_step % 5000 == 0:
                print(f"    DEBUG: MADDPG actions shape = {maddpg_actions.shape}")
                print(f"    DEBUG: Sample action = {maddpg_actions[0]} (agent {agent_ids[0]})")

            # --- ç¯å¢ƒäº¤äº’ ---
            next_observations, rewards, terminated, truncated, info = env.step(actions)

            # --- 4. MADDPGç»éªŒå­˜å‚¨ ---
            next_states = []
            reward_list = []
            done_list = []
            
            for agent_id in agent_ids:
                next_states.append(next_observations[agent_id])
                reward_list.append(rewards[agent_id])
                done_list.append(terminated or truncated)
                
            next_states = np.array(next_states)
            reward_list = np.array(reward_list)
            done_list = np.array(done_list)
            
            # å­˜å‚¨è½¬æ¢åˆ°MADDPGçš„ç»éªŒå›æ”¾ç¼“å†²åŒº
            maddpg.store_transition(states, maddpg_actions, reward_list, next_states, done_list)
            
            # ä¸ºå…¼å®¹æ€§ç»´æŠ¤PPO bufferï¼ˆå®é™…ä¸ä½¿ç”¨ï¼‰
            for agent_id, ppo_agent in ppo_agents.items():
                ppo_agent.buffer.rewards.append(rewards[agent_id])
                ppo_agent.buffer.is_terminals.append(terminated or truncated)

            observations = next_observations
            current_episode_reward += sum(rewards.values())
            
            # ç»Ÿè®¡æ£€æµ‹äº‹ä»¶
            if 'detected_events' in info:
                episode_detected_count += len([event for event in info['detected_events'] 
                                             if 'successfully detected' in event])

            # --- 5. MADDPGæ¨¡å‹æ›´æ–° ---
            if time_step % config.UPDATE_TIMESTEPS == 0:
                print(f"    MADDPG UPDATING at time_step {time_step}...")
                maddpg.update()
                print(f"    MADDPG UPDATE finished.")
                
                # æ‰“å°è®­ç»ƒç»Ÿè®¡
                if len(maddpg.agents[0].actor_loss_history) > 0:
                    avg_actor_loss = np.mean([agent.actor_loss_history[-10:] for agent in maddpg.agents])
                    avg_critic_loss = np.mean([agent.critic_loss_history[-10:] for agent in maddpg.agents])
                    print(f"    Avg Actor Loss: {avg_actor_loss:.4f}, Avg Critic Loss: {avg_critic_loss:.4f}")
            
            # --- è®­ç»ƒç›‘æ§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰---
            if config.ENABLE_RENDERING:
                env.render()
                clock.tick(config.TARGET_FPS)
            elif time_step % 1000 == 0:  # æ¯1000æ­¥ç›‘æ§ä¸€æ¬¡
                avg_reward = current_episode_reward / episode_steps if episode_steps > 0 else 0
                print(f"    Step {time_step}, Episode Steps: {episode_steps}, Avg Reward: {avg_reward:.3f}, Episode Reward: {current_episode_reward:.1f}")
                
                # ä½¿ç”¨æ–°çš„infoä¿¡æ¯
                detection_rate = info.get('detection_rate', 0)
                detected_count = info.get('detected_count', 0)
                total_spawned = info.get('total_spawned', 0)
                print(f"    æ£€æµ‹è¿›åº¦: {detected_count}/{total_spawned} ({detection_rate:.1%}), Episode total: {episode_detected_count}")
                
                # æ˜¾ç¤ºæ™ºèƒ½ä½“çŠ¶æ€ï¼ˆåŒ…å«æ‰€æœ‰æ™ºèƒ½ä½“ï¼‰
                all_agents = [(agent.id, agent.pos) for agent in env.agents]
                print(f"    æ™ºèƒ½ä½“ä½ç½®: {[(aid, f'({pos[0]:.0f},{pos[1]:.0f})') for aid, pos in all_agents[:3]]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
                
                # æ˜¾ç¤ºå½“å‰å¥–åŠ±ç»„æˆï¼ˆè°ƒè¯•ç”¨ï¼‰
                recent_rewards = {aid: rewards.get(aid, 0) for aid in list(rewards.keys())[:2]}
                print(f"    Recent rewards sample: {recent_rewards}")
                print(f"    Episode time: {env.current_time:.1f}s, Time penalty={config.REWARD_TIME_STEP}")
                print("-" * 60)

        # å›åˆç»“æŸç»Ÿè®¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        episode_rewards.append(current_episode_reward)
        detection_stats.append(episode_detected_count)
        
        # è·å–ç»ˆæ­¢ä¿¡æ¯
        termination_reason = info.get('termination_reason', 'unknown')
        detection_rate = info.get('detection_rate', 0)
        total_spawned = info.get('total_spawned', 0)
        
        # å›åˆæ‘˜è¦
        print(f"Episode {episode + 1} Summary:")
        print(f"  Total Reward: {current_episode_reward:.1f}")
        print(f"  Targets Detected: {episode_detected_count}/{total_spawned} ({detection_rate:.1%})")
        print(f"  Steps: {episode_steps}")
        print(f"  Episode Time: {env.current_time:.1f}s")
        print(f"  Termination Reason: {termination_reason}")
        print(f"  Average Reward/Step: {current_episode_reward/episode_steps:.2f}")
        
        # æ¯10å›åˆæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        if (episode + 1) % 10 == 0:
            recent_rewards = episode_rewards[-10:]
            recent_detections = detection_stats[-10:]
            avg_reward = sum(recent_rewards) / len(recent_rewards)
            avg_detections = sum(recent_detections) / len(recent_detections)
            
            print(f"\n{'='*50}")
            print(f"MADDPG TRAINING PROGRESS - Episode {episode + 1}")
            print(f"{'='*50}")
            print(f"Last 10 episodes average reward: {avg_reward:.1f}")
            print(f"Last 10 episodes average detections: {avg_detections:.1f}")
            print(f"Best episode reward so far: {max(episode_rewards):.1f}")
            print(f"Best detection count so far: {max(detection_stats)}")
            
            # å­¦ä¹ è¶‹åŠ¿åˆ†æ
            if len(episode_rewards) >= 20:
                first_half = sum(episode_rewards[-20:-10]) / 10
                second_half = sum(episode_rewards[-10:]) / 10
                trend = "â†—ï¸ Improving" if second_half > first_half else "â†˜ï¸ Declining" if second_half < first_half else "â†’ Stable"
                print(f"Learning trend: {trend} ({second_half:.1f} vs {first_half:.1f})")
            
            # ç½‘ç»œè®­ç»ƒçŠ¶æ€
            if len(maddpg.agents[0].actor_loss_history) > 10:
                recent_actor_loss = np.mean([agent.actor_loss_history[-10:] for agent in maddpg.agents if len(agent.actor_loss_history) >= 10])
                recent_critic_loss = np.mean([agent.critic_loss_history[-10:] for agent in maddpg.agents if len(agent.critic_loss_history) >= 10])
                print(f"Recent training losses: Actor={recent_actor_loss:.4f}, Critic={recent_critic_loss:.1f}")
                
            print(f"Buffer size: {len(maddpg.replay_buffer)}")
            print(f"{'='*50}\n")
    
    # è®­ç»ƒå®Œæˆæ‘˜è¦
    print("\n" + "ğŸ¯ TRAINING COMPLETED ğŸ¯".center(60, "="))
    print(f"Total episodes: {num_episodes}")
    print(f"Best episode reward: {max(episode_rewards):.1f}")
    print(f"Final 10 episodes average: {sum(episode_rewards[-10:]) / 10:.1f}")
    print(f"Best detection performance: {max(detection_stats)} targets")
    print(f"Average detections (final 10): {sum(detection_stats[-10:]) / 10:.1f}")
    print("=" * 60)
    
    env.close()
    print("Simulation finished.")

if __name__ == "__main__":
    main()